\section{Low-rank matrix factorization methods}
\label{sxn:low-rank-methods}
%% \textit{Owners: Jiyan, Michael Mahoney (1 page)}

Given an $m \times n$ data matrix $A$, low-rank matrix factorization methods
aim to find two smaller matrices whose product is a good approximation to $A$.
That is, they aim to find matrices $Y$ and $Z$ such that
\begin{equation}
 \label{eqn:apprx}
    \underset{m\times n}{A} \approx \underset{m\times k}{Y} \times \underset{k\times n}{Z} , 
\end{equation}
where $Y \times Z$ is a rank-$k$ approximation to the original matrix $A$.
Low-rank matrix factorization methods are ubiquitous in linear algebra
and numerical analysis, and they find use in a variety of scientific fields and
scientific computing as well as in machine learning and data analysis
applications such as pattern recognition and personalized recommendation.

Depending on the application, various low-rank factorization techniques are of
interest.  Popular choices include the singular value
decomposition~\cite{GVL96}, principal component analysis~\cite{pcaBook},
rank-revealing QR factorization~\cite{GE96}, nonnegative matrix
factorization~\cite{NMFalg}, and CUR/CX decompositions~\cite{CUR_PNAS}.  In
this work, we consider using the PCA and randomized SVD decompositions for scalable 
scientific data analysis; in the remainder of this section, we briefly
describe these decompositions.
%and previous implementations of low-rank factorizations from the HPC and data analytics communities. 
For an arbitrary matrix $A$, denote by $\a_i$
its $i$-th row, $\a^j$ its $j$-th column and $\a_{ij}$ its $(i,j)$-th element.
Throughout, we assume $A$ has size $m \times n$ and rank $r$.


\subsection{SVD and PCA}

The singular value decomposition (SVD) is the factorization of $A \in
\reals^{m\times n}$ into the product of three matrices $U\Sigma V^T$ where $U
\in \reals^{m\times r}$ and $V\in \reals^{n\times r}$ have orthonormal columns
and $\Sigma\in \reals^{r\times r}$ is a diagonal matrix with positive real
entries. The columns of $U$ and $V$ are called left and right singular vectors
and the diagonal entries of $\Sigma$ are called singular values.  For notation
convenience, we assume the singular values are sorted such that $\sigma_1\geq
\cdots \geq \sigma_r\geq 0$, and that the columns of $U$ and $V$ are
sorted by the order given by the singular values.  
 
The SVD is of central interest because it provides the ``best'' low-rank matrix
approximation with respect to any unitarily invariant matrix norm.  In
particular, for any target rank $k \leq r$, the SVD provides the minimizer of
the optimization problem
\begin{equation}
 \label{eqn:obj}
  \min_{\text{rank}(\tilde A) = k} \| A - \tilde A \|_F,
\end{equation}
where the Frobenius norm $\| \cdot \|_F$ is defined as $\|X\|_F^2 =
\sum_{i=1}^m \sum_{j=1}^n X_{ij}^2 $. Specifically, the solution
to~\eqref{eqn:obj} is given by the truncated SVD, i.e., $A_k = U_k \Sigma_k
V_k^T$, where the columns of $U_k$ and $V_k$ are the top $k$ singular vectors,
i.e., the first $k$ columns of $U$ and $V$, respectively, and $\Sigma_k$ is a 
diagonal matrix containing the top-$k$ singular values.

Principal component analysis (PCA) and SVD are closely related.  PCA aims to
convert the original features into a set of orthogonal directions called {\it
principal components} that capture most of the variance in the data points.
The PCA decomposition of $A$ is given by the SVD of the matrix formed by
centering each column of $A$ (i.e., removing the mean of each column).  When
low-rank methods are appropriate, the number of principal components needed to
preserve most of the information in $A$ is far less than the number of original
features, and thus the goal of dimension reduction is achieved.

The core of our PCA/SVD decompositions is the Implicitly Restarted Arnoldi Method~\cite{ArpackUserGuide}
for finding eigendecompositions of a symmetric matrix.  This is an iterative
method that works by finding increasingly accurate approximations to the top
few eigenvalues and eigenvectors of $A^TA$, and can be terminated when the
approximations are acceptably accurate. IRAM is amenable to distributed
computing as it does not require access to the entries of $A$, instead it
requires only the ability to compute the application of $A$ to any vector. It is 
classical that one can obtain the truncated PCA/SVD decomposition of $A$ from that of $A^TA;$
details are provided in Algorithm~\ref{alg:thinSVD}. In our implementation, we use a Java binding of the 
Parallel ARPACK library to compute the IRAM primitive~\cite{maschho1996portable}.

\subsection{Randomized SVD decompositions}

The PCA decomposition is useful when the principal components are the objects of interest, and they are
desired to be known to a high degree of accuracy. When instead a low-rank approximation of $A$ is desired,
the randomized linear algebra community has developed algorithms which use randomized projections to
construct rank-$k$ approximations to $A$ that approximate $A$ nearly as well as $A_k$ does. Importantly,
these algorithms run in $\mathcal{O}(mn k)$ time and need only a constant number of passes over the data matrix.

\begin{algorithm}[tb]
  \caption{\textsc{ThinSVD} Algorithm}
  \label{alg:thinSVD}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, rank parameter $r \leq \textrm{rank}(A).$
    \Ensure $U \Sigma V^T = \textsc{ThinSVD}(A, r).$
    \State Let $(\tilde{V}, \tilde{\Sigma}) = \textsc{IRAM}(\textsc{MultiplyGramian}(A, \cdot), r).$
    \State Let $Y = \textsc{Multiply}(A, \tilde{V}).$
    \State Compute $(Q, R) = \textsc{ThinQR}(Y).$
    \State Compute $(\tilde{U}, \Sigma, V^T) = \textsc{ThinSVD}(R \tilde{V}^T).$
    \State Let $U = Q \tilde{U}.$  
  \end{algorithmic}
\end{algorithm}

We use an algorithm introduced in \cite{MRT06,MRT11}.
This algorithm, which
we refer to as \textsc{RandomizedSVD}, is summarized in
Algorithm~\ref{alg:rsvd}.  (Algorithm~\ref{alg:rsvd} calls
\textsc{MultiplyGramian}, which is summarized in Algorithm~\ref{alg:gram}, as
well as three algorithms, \textsc{Multiply}, \textsc{ThinQR}, and
\textsc{ThinSVD}, which are standard in numerical linear algebra~\cite{GVL96}.)

Intuitively, Steps 1--6 of Algorithm~\ref{alg:rsvd} find a randomized approximation of the
top right singular vectors of $A$. Since $Q$ is a good approximation to the right singular vectors of
$A$, the matrix $AQQ^T$ is a good low-rank approximation to $A$. Steps 7--9 calculate the SVD
of this low-rank approximation without explicitly forming $AQQ^T.$ 
In terms of the parameters $r$ and $\ell$, it is known that the approximation error of Algorithm~\ref{alg:rsvd} satisfies
\[
  \|A - U\Sigma V^T\|_F \leq \left(1 + \frac{r}{\ell - 1}\right)^{1/2} \|A - A_r\|_F 
\]
on average~\cite{HMT09_SIREV}. Similar guarantees hold with high probability, and as $q$ increases, the error of the approximation decreases; we refer the readers
to~\cite{HMT09_SIREV,Mah-mat-rev_BOOK} for more details. 

The running time cost for \textsc{RandomizedSVD} is dominated by the
matrix-matrix multiplication, which involve passing over the entire data
matrix, appearing in Step 3 and Step 7 of Algorithm~\ref{alg:rsvd}.  These
steps can be parallelized, and hence \textsc{RandomizedSVD} is amenable to
distributed computing.



\begin{algorithm}[tb]
 \caption{{\sc RandomizedSVD} Algorithm}
  \label{alg:rsvd}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, \
      number of power iterations $q \ge 1$, \
      target rank $r > 0$, slack $\ell \ge 0$, and let $k=r+\ell$.

    % TODO: domain of each result
    \Ensure $U \Sigma V^T \approx \Call{ThinSVD}{A, r}$.

    \State Initialize $B \in \reals^{n\times k}$ by sampling $B_{ij} \sim \mathcal{N}(0, 1)$.

    \For{$q$ times}
        \State $B \gets \Call{MultiplyGramian}{A, B}$
        \State $(B, \_) \gets \Call{ThinQR}{B}$
    \EndFor

    \State Let $Q$ be the first $r$ columns of $B$.

    \State Let $C = \Call{Multiply}{A, Q}$.

    \State Compute $(U, \Sigma, \tilde V^T) = \Call{ThinSVD}{C}$.

    \State Let $V = Q \tilde V$.

    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
  \caption{{\sc MultiplyGramian} Algorithm}
  \label{alg:gram}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, $B \in \reals^{n\times k}$.
    \Ensure $X = A^T A B$.
    \State Initialize $X = 0$.
    \For{each row $a$ in $A$}
        \State $X \gets X + a a^T B$.
    \EndFor
  \end{algorithmic}
\end{algorithm}


%Finally, we want to point out that,
%although delivering different low-rank factorizations, both PAC and CX suffer from the fact that they need to truncated SVD.
%To make the algorithms practical on large-scale dataset, one can alleviate the demanding complexity by using randomized SVD. 

%These algorithms compute high-quality approximations to the normalized leverage scores of the input matrix, and the running time of these algorithms depends on the time to apply a random projection to the input matrix, which is much faster than computing the full (or even a truncated) SVD.
