\section{Low-rank matrix factorization methods}
\label{sxn:low-rank-methods}
%% \textit{Owners: Jiyan, Michael Mahoney (1 page)}

Given an $m \times n$ data matrix $A$, low-rank matrix factorization methods aim to find two smaller matrices whose product is a good approximation to $A$.
That is, they aim to find matrices $Y$ and $Z$ such that
\begin{equation}
 \label{eqn:apprx}
    \underset{m\times n}{A} \approx \underset{m\times k}{Y} \times \underset{k\times n}{Z} , 
\end{equation}
where $Y \times Z$ is a rank-$k$ approximation to the original matrix $A$.
Low-rank matrix factorization methods are an important topic in linear algebra and numerical analysis, and they find use in a variety of scientific fields and scientific computing as well as in machine learning and data analysis applications such as pattern recognition and personalized recommendation.

Depending on the application, various low-rank factorization techniques are of interest. 
Popular choices include the singular value decomposition~\cite{GVL96}, principal component analysis~\cite{pcaBook}, rank-revealing QR factorization~\cite{GE96}, nonnegative matrix factorization~\cite{NMFalg}, and CUR/CX decompositions~\cite{CUR_PNAS}.
In this work, we consider using the SVD and CX decompositions for scalable and interpretable data analysis; in the remainder of this section, we briefly describe these decompositions.
For an arbitrary matrix $A$, denote by $\a_i$ its $i$-th row, $\a^j$ its $j$-th column and $\a_{ij}$ its $(i,j)$-th element. 
Throughout, we assume $A$ has size $m \times n$ and rank $r$.


\subsection{SVD and PCA}

The singular value decomposition (SVD) is the factorization of $A \in \reals^{m\times n}$ into the product of three matrices $U\Sigma V^T$ where $U \in \reals^{m\times r}$ and $V\in \reals^{n\times r}$ have orthonormal columns and $\Sigma\in \reals^{r\times r}$ is a diagonal matrix with positive real entries. 
The columns of $U$ and $V$ are called left and right singular vectors and the diagonal entries of $\Sigma$ are called singular values. 
For notational convenience, we assume the singular values are sorted such that $\sigma_1\geq \cdots \geq \sigma_r\geq 0$.

%if optimality criterion to choose is the square loss ($\ell_2$ loss)
 
The SVD is of central interest because it provides the ``best'' low-rank matrix approximation with respect to any unitarily invariant matrix norm.
In particular, for any target rank $k \leq r$, the SVD provides the minimizer of the optimization problem
\begin{equation}
 \label{eqn:obj}
  \min_{\text{rank}(\tilde A) = k} \| A - \tilde A \|_F,
\end{equation}
where the Frobenius norm $\| \cdot \|_F$ is defined as $\|X\|_F^2 =
\sum_{i=1}^m \sum_{j=1}^n X_{ij}^2 $. Specifically, the solution
to~\eqref{eqn:obj} is given by the truncated SVD, i.e., $A_k = U_k \Sigma_k
V_k^T$, where the columns of $U_k$ and $V_k$ are the top $k$ singular vectors,
i.e., the first $k$ columns of $U$ and $V$, respectively, and $\Sigma_k$ is a 
diagonal matrix containing the top-$k$ singular values.

Principal component analysis (PCA) and SVD are closely related.  PCA aims to
convert the original features into a set of orthogonal directions called {\it
principal components} that capture most of the variance in the data points.
The PCA decomposition of $A$ is given by the SVD of the matrix formed by
centering each column of $A$ (i.e., removing the mean of each column).  When
low-rank methods are appropriate, the number of principal components needed to
preserve most of the information in $A$ is far less than the number of original
features, and thus the goal of dimension reduction is achieved.

\subsection{Randomized SVD}

The computation of the SVD (and thus of PCA) for a data matrix $A$ is
expensive~\cite{GVL96}.  For example, to compute the truncated SVD with rank
$k$ using traditional deterministic methods, the running time complexity is
$\bigO(mnk)$, and $\bigO(k)$ passes over the dataset are needed.  This becomes
prohibitively expensive when dealing with datasets of even moderately-large
size, e.g., $m = 10^6$, $n = 10^4$ and $k = 20$.  To address these and related
issues, recent work in Randomized Linear Algebra (RLA) has focused on using
randomized approximation to perform scalable linear algebra computations for
large-scale data problems.  For an overview of the RLA area,
see~\cite{Mah-mat-rev_BOOK}; for a review of using RLA methods for low-rank
matrix approximation, see~\cite{HMT09_SIREV}; and for a review of the theory
underlying implementing RLA methods in parallel/distributed environments,
see~\cite{YMM15_TR}.

Here, we will use an algorithm introduced in \cite{MRT06,MRT11}
that uses a random projection to construct a rank-$k$ approximation to $A$
which approximates $A$ nearly as well as $A_k$ does.  We refer the readers to
\cite{HMT09_SIREV,Mah-mat-rev_BOOK} for more details.  Importantly, the
algorithm runs in $\bigO(mn \log k)$ time, and the algorithm needs only a
constant number of passes over the data matrix.  These properties becomes
extremely desirable in many large-scale data analytics.  This algorithm, which
we refer to as \textsc{RandomizedSVD}, is summarized in
Algorithm~\ref{alg:rsvd}.  (Algorithm~\ref{alg:rsvd} calls
\textsc{MultiplyGramian}, which is summarized in Algorithm~\ref{alg:gram}, as
well as three algorithms, \textsc{Multiply}, \textsc{ThinQR}, and
\textsc{ThinSVD}, which are standard in numerical linear algebra~\cite{GVL96}.)
The running time cost for \textsc{RandomizedSVD} is dominated by the
matrix-matrix multiplication, which involve passing over the entire data
matrix, appearing in Step 3 and Step 7 of Algorithm~\ref{alg:rsvd}.  
These steps can be parallelized, and hence \textsc{RandomizedSVD} is 
amenable to distributed computing.
% \footnote{We should mention that Step 3 of
%Algorithm~\ref{alg:rsvd} involves a dense matrix-matrix multiplication, whereas
%most RLA algorithms, including the variant of \textsc{RandomizedSVD}
%in~\cite{HMT09_SIREV} uses an FFT-based random projection in order to obtain a
%faster asymptotic running time.  While our version incurs more FLOPS, it is
%more parallelizable and thus more suitable for distributed computational
%environments.}


\begin{algorithm}[tb]
 \caption{{\sc RandomizedSVD} Algorithm}
  \label{alg:rsvd}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, \
      number of power iterations $q \ge 1$, \
      target rank $r > 0$, slack $\ell \ge 0$, and let $k=r+\ell$.

    % TODO: domain of each result
    \Ensure $U \Sigma V^T \approx \Call{ThinSVD}{A, r}$.

    \State Initialize $B \in \reals^{n\times k}$ by sampling $B_{ij} \sim \mathcal{N}(0, 1)$.

    \For{$q$ times}
        \State $B \gets \Call{MultiplyGramian}{A, B}$
        \State $(B, \_) \gets \Call{ThinQR}{B}$
    \EndFor

    \State Let $Q$ be the first $r$ columns of $B$.

    \State Let $C = \Call{Multiply}{A, Q}$.

    \State Compute $(U, \Sigma, \tilde V^T) = \Call{ThinSVD}{C}$.

    \State Let $V = Q \tilde V$.
    % alternative that doesn't require knowing the distributivity of transpose but looks silly:
    %\State Let $V^T = \tilde V^T Q^T$.

    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
  \caption{{\sc MultiplyGramian} Algorithm}
  \label{alg:gram}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, $B \in \reals^{n\times k}$.
    \Ensure $X = A^T A B$.
    \State Initialize $X = 0$.
    \For{each row $a$ in $A$}
        \State $X \gets X + a a^T B$.
    \EndFor
  \end{algorithmic}
\end{algorithm}


\subsection{CX/CUR decompositions}

In addition to developing improved algorithms for PCA/SVD and related problems,
work in RLA has also focused on so-called CX/CUR
decompositions~\cite{DMM08,CUR_PNAS}.  As a motivation, observe that singular
vectors are eigenvectors of the Gram matrix $A^TA$, and thus they are linear
combinations of up to all of the original variables.  A natural question
arises: can we reconstruct the matrix using a small number of actual columns of
$A$?

CX/CUR decompositions affirmatively answer this question.  That is, these are
low-rank matrix decompositions that are expressed in terms of a small number of
actual columns/rows. As such, they have found applicability in scientific
applications where coupling analytical techniques with domain knowledge is at a
premium, including genetics~\cite{Paschou07b}, astronomy~\cite{Yip14-AJ}, and
mass spectrometry imaging~\cite{YRPMB15}.

In more detail, CX decomposition factorizes an $m \times n$ matrix $A$ into two
matrices $C$ and $X$, where $C$ is an $m\times c$ matrix that consists of $c$
actual columns of $A$, and $X$ is a $c \times n$ matrix such that $A\approx
CX$.
(CUR decompositions further choose $X=UR$, where $R$ is a small number of actual rows
of $A$~\cite{DMM08,CUR_PNAS}.)
For CX, using the same optimality criterion defined in~\eqref{eqn:obj}, we seek
matrices $C$ and $X$ such that the residual error $\|A-CX\|_F$ is small.

The algorithm of~\cite{DMM08} that computes a $1\pm\epsilon$ relative-error
low-rank CX matrix approximation consists of three basic steps: first, compute
(exactly or approximately) the {\it statistical leverage scores} of the columns
of $A$; and second, use those scores as a sampling distribution to select $c$
columns from $A$ and form $C$; finally once the matrix $C$ is determined, the
optimal matrix $X$ with rank-$k$ that minimizes $\|A-CX\|_F$ can be computed
accordingly; see~\cite{DMM08} for detailed construction. 

\begin{algorithm}[tb]
 \caption{\textsc{CXDecomposition}}
  \label{alg:cx}
  \begin{algorithmic}[1]
    \Require $A \in \reals^{m\times n}$, rank parameter $k \leq \textrm{rank}(A)$, number of power iterations $q$.

    \Ensure $C$.
    
    %\Function{ColSelect}{$A,k,r$}

    %\State Using algorithm of\cite{DMMW12_JMLR}, compute an approximation of the top-$k$ right singular vectors of $A$ denoted by $\tilde V_k$, using \textsc{RandomizedSVD} with $q$ power iterations.
    \State Compute an approximation of the top-$k$ right singular vectors of $A$ denoted by $\tilde V_k$, using \textsc{RandomizedSVD} with $q$ power iterations.
    
    \State Let $\ell_i = \sum_{j=1}^k \tilde \v_{ij}^2$, where $\tilde \v_{ij}^2$ is the $(i,j)$-th element of $\tilde V_k$, for $i = 1, \ldots, n$. 
    
    \State Define $p_i = \ell_i / \sum_{j=1}^d \ell_j$ for $i=1,\ldots,n$.
    
    \State Randomly sample $c$ columns from $A$ in i.i.d. trials, using the importance sampling distribution $\{p_i\}_{i=1}^n$ .

    %\State \Return The indices with the top $r$ $\ell_i$s.
    
    %\EndFunction

    \end{algorithmic}
\end{algorithm}

The algorithm for approximating the leverage scores is
provided in Algorithm~\ref{alg:cx}.
Let $A=U\Sigma V^T$ be the SVD of $A$.
Given a target rank parameter $k$, for $i=1,\ldots,n$, the $i$-th leverage score is defined as
\begin{equation}
 \label{eqn:lev}
  \ell_i = \sum_{j=1}^k \v_{ij}^2.
\end{equation}
These scores quantify the amount of ``leverage'' each column of $A$ exerts on the best rank-$k$ approximation to $A$. 
For each column of $A$, we have 
  $$  \a_i = \sum_{j=1}^{r} (\sigma_j \u_j) \v_{ij} \approx \sum_{j=1}^k (\sigma_j \u_j) \v_{ij}.  $$
  That is, the $i$-th column of $A$ can be expressed as a linear combination of the basis of the dominant $k$-dimensional left singular space with $\v_{ij}$ as the coefficients.
If, for $i=1,\ldots,n$, we define the {\it normalized leverage scores} as
\begin{equation}
\label{eqn:nlev}
  p_i = \frac{\ell_i}{\sum_{j=1}^n \ell_j},
\end{equation}      
where $\ell_i$ is defined in~\eqref{eqn:lev}, and choose columns from $A$ according to those normalized leverage scores, then (by~\cite{DMM08,CUR_PNAS}) the selected columns are able to reconstruct the matrix $A$ nearly as well as $A_k$ does.

The running time for \textsc{CXdecomposition} is determined by the
computation of the importance sampling distribution.  To compute the leverage
scores based on \eqref{eqn:lev}, one needs to compute the top
$k$ right-singular vectors $V_k$. This can be prohibitive on
large matrices.  However, we can once again
use \textsc{RandomizedSVD} to compute {\it approximate} leverage scores. This
approach, originally proposed by Drineas et al.~\cite{DMMW12_JMLR}, runs in
``random projection time,'' so requires fewer FLOPS and fewer passes over
the data matrix than deterministic algorithms that compute the
leverage scores~exactly.


%Finally, we want to point out that,
%although delivering different low-rank factorizations, both PAC and CX suffer from the fact that they need to truncated SVD.
%To make the algorithms practical on large-scale dataset, one can alleviate the demanding complexity by using randomized SVD. 

%These algorithms compute high-quality approximations to the normalized leverage scores of the input matrix, and the running time of these algorithms depends on the time to apply a random projection to the input matrix, which is much faster than computing the full (or even a truncated) SVD.
