\section{Results}
\label{sec:results}

%Performance Evaluation, Results, Discussion}

%\textit{owners: Evan: graphs, All: interpretation (2.75 pages)}

\subsection{CX Performance using C and MPI} %%on Single Node}
  \label{sxn:results1}


   
%%  \vspace*{0.1in}

      In Table~\ref{tab:single_node}, we show the benefits of various
      optimizations described in
      Sec.~\ref{sxn:single_node_opt}. 
      %%%%
      %%%%
      %%%%
      %to the performance of \textsc{MultiplyGramian} and \textsc{Multiply} on each compute node. 
      %%
      %%
      %The test matrix $\mathcal{A}$ has {\it{m}} = 1.95M, {\it{n}} = 128K,
      %{\it{s}} = 0.004, and {\it{nnz}} = 10$^9$. The parameter
      %{\it{k}} = 32. 
      As far as single-node performance is concerned, we started with a parallelized implementation  
      without any of the described optimizations. %%, and measured the performance (in terms of time taken). 
      We first implemented the multi-core synchronization scheme, wherein a single copy of the
      output matrix is maintained, %% across all the threads (for the matrix multiplication).
      which resulted in a speedup of 6.5X, primarily due to
      the reduction in the amount of data traffic between 
      main memory and caches. 
      %%last-level cache and main memory (there was around 19X measured reduction in traffic). 
      We then implemented our cache blocking scheme, which led to a
      further  2.4X speedup (overall 15.6X).
%%      primarily targeted towards ensuring that the output of the matrix multiplication resides in the caches (since it is
%      accessed and updated frequently). This led to a further 2.4X reduction in run-time, for an overall speedup of around 15.6X.
      We then implemented our SIMD code that sped it up by a further
      2.6X, for an overall speedup of 39.7X. 



%%     Once the memory traffic was optimized for, we implemented our
%%     SIMD code by vectorizing the element-row multiplication-add
%%     operations (described in detail in Sec.~\ref{sxn:single_node_opt}). 
%%     The resultant code sped up by a further 2.6X, for an overall
%%     speedup of 39.7X. Although the effective SIMD width
%% ($\mathcal{S}$ = 4), there are overheads of address computation,
%% stores, and not all computations were vectorized (QR code is still %% scalar).



        As far as the multi-node performance is concerned, 
 on the Amazon EC2 cluster, with 30 nodes (960-cores in total), and
 the 1 TB dataset as input, it
 took 151 seconds to perform CX computation (including time to load
 the data into main memory). 
 As compared to the Scala code on the same platform (details in
 next sec.), we achieve a speedup of 21X.

 Some of these optimizations can be implemented in Spark, such as arranging the
 order of memory accesses to make efficient use of memory. %%of the memory bus.
 However, other optimizations such as sharing the output matrix between threads
 and use of SIMD intrinsics fall outside the Spark programming model, and would
 require piercing the abstractions provided by Spark and JVM.
 %to more directly access and manipulate the hardware.
 Thus there is a tradeoff between optimizing performance 
 and ease of implementation, %% and efficient global scheduling, 
 available by expressing programs in the Spark programming model.

 
  \begin{table}
  \begin{center}
  \begin{tabular}{ |c|c| } 
  \hline
  Single Node Optimization & Overall Speedup\\
  \hline
  Original Implementation & 1.0  \\
  Multi-Core Synchronization & 6.5 \\
  Cache Blocking & 15.6 \\
  SIMD & 39.7 \\
  \hline

  \end{tabular}
  \end{center}
  \caption{Single node opt. to CX C implementation and
  subsequent speedup  each additional optimization provides.}
  \label{tab:single_node}
  \end{table}
 



  \subsection{CX and PCA Performance Using Spark} %% on Spark} across Multiple Nodes}

  \subsubsection{Spark Phases}
  The CX and PCA algorithms proceed in four distributed phases listed below, along with a small amount of additional local computation.
  \begin{enumerate}
      \item \textbf{Load Matrix Metadata}
         The dimensions of the matrix are read from the distributed filesystem to the driver.
      \item \textbf{Load Matrix}
         A distributed read is performed to load the matrix entries into an in-memory cached
         RDD containing one entry per row of the matrix. PCA uses an additional pass over the matrix to compute and remove the column means.
       \item \textbf{Power Iterations / MultiplyGramian}
         In the case of the CX algorithm, the \textsc{MultiplyGramian} loop (lines 2-5) of
         \textsc{RandomizedSVD} is run to compute an approx. $Q$
         of the dominant right singular subspace. For the PCA algorithm, the ARPACK routine calls the \textsc{MultiplyGramian} function
         until the approximate eigenvectors converge to the desired top eigenspace of $AA^T.$
       \item \textbf{Finalization (Post-Processing)}
         In the case of the CX algorithm, right multiplication by $Q$ (line 7) of \textsc{RandomizedSVD} to compute $C$. In the case of the PCA algorithm,
         right multiplication by $V$ followed by some local post-processing to compute the principal components vectors.
  \end{enumerate}

  \subsubsection{Empirical Results}

    \begin{figure} [h!btp]
    \begin{centering}
    \includegraphics[scale=0.4]{images/CX_Strong_Scaling_New_Colors_Axes_Rank_32_Partitions_default.pdf}
    \end{centering}
    \caption{ Strong scaling for the 4 phases of CX on an XC40 for 100GB dataset at $k=32$ and default partitioning as concurrency is increased.} 
    \label{fig:xc40scaling}
    \end{figure} 

Fig.~\ref{fig:xc40scaling} shows how the distributed Spark portion of our CX code scales.
We considered 240, 480, and 960 cores.  An additional doubling (to 1920 cores) would be ineffective as there are only 1654 partitions, 
so many cores would remain unused.  
When we go from 240 to 480 cores, we achieve a speedup of 1.6x:
233 seconds versus 146 seconds.  However, as the number of partitions per core drops 
below two, and the amount of computation-per-core relative to communication overhead drops, 
the scaling slows down (as expected).  
This results in a lower speedup of 1.4x (146 seconds versus 102
seconds) from 480 to 960 cores.
We did not produce a similar scaling plot for the PCA experiments, as using less
than the maximum 960 cores results in the dataset (2.2 TB of climate data)
results in the dataset not fitting in memory.

\subsection{CX and PCA Performance across Multiple Platforms}
  \label{sect:h2h}
    
    \begin{figure} [h!btp]
    \begin{centering}
      \includegraphics[scale=0.4]{images/CX_Size_Scaling_EXP_CC_xc40_ec2_Rank_16_and_32_Partitions_default.pdf}
    \end{centering}
    \caption{ Run times for the various stages of computation of CX on the three platforms using $k=16$ and $k=32$ on the 1 TB size dataset, using the default partitioning on each platform.} 
    \label{fig:h2hrank16} 
    \end{figure}

    \begin{figure} [h!btp]
      \begin{centering}
        \includegraphics[scale=0.3]{images/phase_stackplot_ec2_ranks_10_and_20}
      \end{centering}
      \caption{Run times for the various stages of computation of PCA on the EC2 and Cori platforms using $k=10$ and $k=20$ on the 2.2 TB size dataset, using the default partitioning.}
      \label{fig:pca_h2hranks}
    \end{figure}
    
  \input{h2hresults.tex}

  \subsection{MSI Science Results}
  
  \begin{figure}[h!bt]
    \centering
    \includegraphics[width=.9\columnwidth]{images/cx_ions.pdf}
      \caption{Normalized leverage scores (sampling probabilities) for $m/z$ marginalized over $\tau$.
        Three narrow regions of $m/z$ account for $59.3\%$ of the total probability mass.}
      \label{fig:cx_ions}
  \end{figure} 

  The rows and columns of our data matrix $A$ correspond to pixels and $(\tau, m/z)$ values of ions, respectively. 
  We compute the CX decompositions of both $A$ and $A^T$ in order to identify important ions in addition to important pixels.
   
  In Figure~\ref{fig:cx_ions}, we present the distribution of the normalized
  ion leverage scores marginalized over $\tau$. That is, each score corresponds
  to an ion with $m/z$ value shown in the $x$-axis. Leverage scores of ions in
  three narrow regions have significantly larger magnitude than the rest. This
  indicates that these ions are more informative and should be kept as basis
  for reconstruction.  Encouragingly, several other ions with significant
  leverage scores are chemically related to the ions with highest leverage
  scores.  For example, the ion with an $m/z$ value of 453.0983 has the second
  highest leverage score among the CX results.  Also identified as having
  significant leverage scores are ions at $m/z$ values of 439.0819, 423.0832,
  and 471.1276, which correspond to neutral losses of $\rm{CH_2}$,
  $\rm{CH_2O}$, and a neutral ``gain'' of $\rm{H_2O}$ from the 453.0983 ion.
  These relationships indicate that this set of ions, all identified by CX as
  having significant leverage scores, are chemically related.  That fact
  indicates that these ions may share a common biological origin, despite
  having distinct spatial distributions in the plant tissue sample.
  

  \subsection{Climate Science Results}

  \begin{figure}[h!bt]
    \centering
    \includegraphics[width=.9\columnwidth]{images/climate_timeseries.pdf}
    \caption{In order from top to bottom, the timeseries for modes 3, 6, and 7 of the CFSR ocean temperature field.}
        \label{fig:climate_timeseries}
  \end{figure} 

  \begin{figure}[h!bt]
    \centering
    \includegraphics[width=.9\columnwidth]{images/climate_spectra.pdf}
    \caption{The power spectra of the first 20 modes of the CFSR ocean temperature field.}
    \label{fig:climate_spectra}
  \end{figure} 
The rows and columns of our data matrix $A$ correspond to observation intervals
and (lat, lon, depth) points, respectively. The first two EOFs fully capture the annual cycles, so in
Figure~\ref{fig:climate_timeseries}, we present the time series corresponding to the
third, sixth, and seventh modes. The time series show the chaotic nature of the
large-scale modes of variability in the ocean, as well as the dominant
periodicities. Further, they show abrupt changes due to the 1983 ENSO,
and more significantly, the record-breaking ENSO of 1997--98.

The power spectra in Figure~\ref{fig:climate_spectra} show which modes contribute to
the low and high frequency modes of variability, and their relative dominance.
Note that, as mentioned above, the first two modes fully capture the annual
cycle, while the higher modes contain low frequency content. The intermediate
modes contain a complex interplay of various timescales, which is currently
under investigation.  

\begin{figure*}[h!bt]
  \centering
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF3_0.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF6_0.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF7_0.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF3_55.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF6_55.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF7_55.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF3_135.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF6_135.pdf}
  \end{subfigure}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{images/EOF7_135.pdf}
  \end{subfigure}
  \caption{Left to right, the mode 3, 6, and 7 spatial EOFs of the CFSR ocean temperature field at, from top to bottom, the surface, a depth of 55 meters, and a depth of 135 meters. }
  \label{fig:spatialEOFs}
\end{figure*}

The spatial EOF patterns in Figure~\ref{fig:spatialEOFs} show the relative
dominance of the Indian Ocean Dipole and the classic warm pool -- cold tongue
patterns of ENSO at various depths below the ocean surface. Note that the
dynamic near the thermocline is most dominant, rather than that closer to the
surface. Further, note that there are several smaller scale features that
have a strong influence at different depths. Work is on going to understand
the nature of these different spatial patterns and the factors that influence
their relative dominance.

  \subsection{Improving Spark on HPC Systems}

  \label{sect:lessons}
  
  \input{lessons.tex}

